{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FCN_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVEquq76CEwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip idd-lite_1.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YeOSp-TCfp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2, os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras, sys, time, warnings\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IiQ4zoJCli-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = 'idd-lite_1/idd20k_lite/'\n",
        "\n",
        "img_train = data + 'leftImg8bit/train/'\n",
        "seg_train = data + 'gtFine/train/'\n",
        "\n",
        "img_val = data + 'leftImg8bit/val/'\n",
        "seg_val = data + 'gtFine/val/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikyYhzS0CyqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mi, ma = 0, 6\n",
        "n_classes = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVVwqKM6C1vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getImageArr(path, width, height):\n",
        "  img = cv2.imread(path, 1)\n",
        "  img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\n",
        "  return img\n",
        "\n",
        "def getSegmentationArr(path, nClasses, width, height):\n",
        "  seg_labels = np.zeros((height, width, nClasses))\n",
        "  img = cv2.imread(path, 1)\n",
        "  img = cv2.resize(img, (width, height))\n",
        "  img = img[:, :, 0]\n",
        "\n",
        "  for c in range(nClasses):\n",
        "      seg_labels[:, :, c] = (img == c ).astype(int)\n",
        "\n",
        "  return seg_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EUP71sxC4_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_height, input_width = 224, 224\n",
        "output_height, output_width = 224, 224"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_A-OfW-C7JH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = [], []\n",
        "X_val, y_val = [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "178Oae30C86p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_img = os.listdir(img_train)\n",
        "train_img.sort()\n",
        "train_seg = os.listdir(seg_train)\n",
        "train_seg.sort()\n",
        "\n",
        "val_img = os.listdir(img_val)\n",
        "val_img.sort()\n",
        "val_seg = os.listdir(seg_val)\n",
        "val_seg.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OYlnU32y0Ei",
        "colab_type": "code",
        "outputId": "cbab54ce-e175-4ea4-a5b5-f01802ea5864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print(train_img[: 5])\n",
        "print(train_seg[: 5])\n",
        "print(val_img[: 5])\n",
        "print(val_seg[: 5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0000002_image.jpg', '0000097_image.jpg', '0000192_image.jpg', '0000215_image.jpg', '0000247_image.jpg']\n",
            "['0000002_label.png', '0000097_label.png', '0000192_label.png', '0000215_label.png', '0000247_label.png']\n",
            "['0000000_image.jpg', '000065_image.jpg', '0001080_image.jpg', '000190_image.jpg', '0001923_image.jpg']\n",
            "['0000000_label.png', '000065_label.png', '0001080_label.png', '000190_label.png', '0001923_label.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81czJBxBC_Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for im, seg in zip(train_img, train_seg):\n",
        "  X_train.append(getImageArr(img_train + im, input_width, input_height))\n",
        "  y_train.append(getSegmentationArr(seg_train + seg, n_classes, output_width, output_height))\n",
        "    \n",
        "for im, seg in zip(val_img, val_seg):\n",
        "  X_val.append(getImageArr(img_val + im, input_width, input_height))\n",
        "  y_val.append(getSegmentationArr(seg_val + seg, n_classes, output_width, output_height))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah6n_lkqDBoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_val, y_val = np.array(X_val),np.array(y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkh4Iymc3TtH",
        "colab_type": "code",
        "outputId": "d66c95c8-9d5b-401d-96b8-9f88b5bdd5ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1380, 224, 224, 3) (1380, 224, 224, 7)\n",
            "(204, 224, 224, 3) (204, 224, 224, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_DesVfzDD32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VGG_Weights_path = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iyWgMV5DFS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def FCN8(nClasses, input_width = 224, input_height = 224):\n",
        "  assert input_width % 32 == 0\n",
        "  assert input_height % 32 == 0\n",
        "  IMAGE_ORDERING = 'channels_last'\n",
        "  \n",
        "  img_input = Input(shape = (input_height, input_width, 3))\n",
        "  \n",
        "  x = Conv2D(64, (3, 3), activation = 'relu', padding = 'same', name = 'block1_conv1', data_format = IMAGE_ORDERING )(img_input)\n",
        "  x = Conv2D(64, (3, 3), activation = 'relu', padding = 'same', name = 'block1_conv2', data_format = IMAGE_ORDERING )(x)\n",
        "  x = MaxPooling2D((2, 2), strides = (2, 2), name = 'block1_pool', data_format = IMAGE_ORDERING )(x)\n",
        "  f1 = x\n",
        "  \n",
        "  x = Conv2D(128, (3, 3), activation = 'relu', padding = 'same', name = 'block2_conv1', data_format = IMAGE_ORDERING )(x)\n",
        "  x = Conv2D(128, (3, 3), activation = 'relu', padding = 'same', name = 'block2_conv2', data_format = IMAGE_ORDERING )(x)\n",
        "  x = MaxPooling2D((2, 2), strides = (2, 2), name = 'block2_pool', data_format = IMAGE_ORDERING )(x)\n",
        "  f2 = x\n",
        "  \n",
        "  x = Conv2D(256, (3, 3), activation = 'relu', padding = 'same', name = 'block3_conv1', data_format = IMAGE_ORDERING )(x)\n",
        "  x = Conv2D(256, (3, 3), activation = 'relu', padding = 'same', name = 'block3_conv2', data_format = IMAGE_ORDERING )(x)\n",
        "  x = Conv2D(256, (3, 3), activation = 'relu', padding = 'same', name = 'block3_conv3', data_format = IMAGE_ORDERING )(x)\n",
        "  x = MaxPooling2D((2, 2), strides = (2, 2), name = 'block3_pool', data_format = IMAGE_ORDERING )(x)\n",
        "  pool3 = x\n",
        "  \n",
        "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block4_conv1', data_format = IMAGE_ORDERING )(x)\n",
        "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block4_conv2', data_format = IMAGE_ORDERING )(x)\n",
        "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block4_conv3', data_format = IMAGE_ORDERING )(x)\n",
        "  x = MaxPooling2D((2, 2), strides = (2, 2), name = 'block4_pool', data_format = IMAGE_ORDERING )(x)\n",
        "  pool4 = x\n",
        "  \n",
        "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block5_conv1', data_format = IMAGE_ORDERING )(pool4)\n",
        "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block5_conv2', data_format = IMAGE_ORDERING )(x)\n",
        "  x = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', name = 'block5_conv3', data_format = IMAGE_ORDERING )(x)\n",
        "  x = MaxPooling2D((2, 2), strides = (2, 2), name = 'block5_pool', data_format = IMAGE_ORDERING )(x)\n",
        "  pool5 = x\n",
        "  \n",
        "  vgg = Model(img_input, pool5)\n",
        "  vgg.load_weights(VGG_Weights_path)\n",
        "  \n",
        "  n = 4096\n",
        "  o = (Conv2D(n, (7, 7), activation = 'relu', padding = 'same', name = 'conv6', data_format = IMAGE_ORDERING))(pool5)\n",
        "  conv7 = (Conv2D(n, (1, 1), activation = 'relu', padding = 'same', name = 'conv7', data_format = IMAGE_ORDERING))(o)\n",
        "  \n",
        "  conv7_4 = Conv2DTranspose(nClasses, kernel_size = (4, 4) ,strides = (4, 4), use_bias = False, data_format = IMAGE_ORDERING )(conv7)\n",
        "  \n",
        "  pool411 = (Conv2D(nClasses, (1, 1), activation = 'relu', padding = 'same', name = 'pool4_11', data_format = IMAGE_ORDERING))(pool4)\n",
        "  pool411_2 = (Conv2DTranspose(nClasses, kernel_size = (2, 2), strides = (2, 2), use_bias = False, data_format = IMAGE_ORDERING ))(pool411)\n",
        "  \n",
        "  pool311 = (Conv2D(nClasses, (1, 1), activation = 'relu', padding = 'same', name = 'pool3_11', data_format = IMAGE_ORDERING))(pool3)\n",
        "      \n",
        "  o = Add(name = 'add')([pool411_2, pool311, conv7_4])\n",
        "  o = Conv2DTranspose(nClasses, kernel_size = (8, 8), strides = (8, 8), use_bias = False, data_format = IMAGE_ORDERING )(o)\n",
        "  o = (Activation('softmax'))(o)\n",
        "  \n",
        "  model = Model(img_input, o)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRxLWMYmDIKA",
        "colab_type": "code",
        "outputId": "6854eff6-6e1d-4ddc-f10f-beb79d4ba651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = FCN8(nClasses = n_classes, input_width  = 224, input_height = 224)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv6 (Conv2D)                  (None, 7, 7, 4096)   102764544   block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "pool4_11 (Conv2D)               (None, 14, 14, 7)    3591        block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv7 (Conv2D)                  (None, 7, 7, 4096)   16781312    conv6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTrans (None, 28, 28, 7)    196         pool4_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_11 (Conv2D)               (None, 28, 28, 7)    1799        block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 28, 28, 7)    458752      conv7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 28, 28, 7)    0           conv2d_transpose_8[0][0]         \n",
            "                                                                 pool3_11[0][0]                   \n",
            "                                                                 conv2d_transpose_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTrans (None, 224, 224, 7)  3136        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 224, 224, 7)  0           conv2d_transpose_9[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 134,728,018\n",
            "Trainable params: 134,728,018\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGXcZ8mmDJ9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = optimizers.SGD(lr = 0.01, decay = 5 ** (-4), momentum = 0.9, nesterov = True)\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = sgd,\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEw5b4HFDSV0",
        "colab_type": "code",
        "outputId": "55f7d190-678e-4823-d80e-e383a618e36b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist1 = model.fit(X_train, y_train,\n",
        "                  validation_data = (X_val, y_val),\n",
        "                  batch_size = 32, epochs = 200, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1380 samples, validate on 204 samples\n",
            "Epoch 1/200\n",
            "1380/1380 [==============================] - 31s 23ms/step - loss: 1.9619 - acc: 0.1501 - val_loss: 1.9417 - val_acc: 0.1535\n",
            "Epoch 2/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 1.9340 - acc: 0.1644 - val_loss: 1.9197 - val_acc: 0.1768\n",
            "Epoch 3/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 1.8480 - acc: 0.2389 - val_loss: 1.6632 - val_acc: 0.3546\n",
            "Epoch 4/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 1.1880 - acc: 0.5751 - val_loss: 1.1458 - val_acc: 0.6277\n",
            "Epoch 5/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.8905 - acc: 0.7082 - val_loss: 0.8732 - val_acc: 0.6975\n",
            "Epoch 6/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.7476 - acc: 0.7299 - val_loss: 0.7980 - val_acc: 0.7142\n",
            "Epoch 7/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.6977 - acc: 0.7430 - val_loss: 0.7074 - val_acc: 0.7359\n",
            "Epoch 8/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.6600 - acc: 0.7560 - val_loss: 0.8527 - val_acc: 0.7022\n",
            "Epoch 9/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.6454 - acc: 0.7611 - val_loss: 0.6712 - val_acc: 0.7471\n",
            "Epoch 10/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.6190 - acc: 0.7699 - val_loss: 0.6515 - val_acc: 0.7547\n",
            "Epoch 11/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.6030 - acc: 0.7755 - val_loss: 0.6323 - val_acc: 0.7597\n",
            "Epoch 12/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5918 - acc: 0.7791 - val_loss: 0.7119 - val_acc: 0.7260\n",
            "Epoch 13/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5804 - acc: 0.7827 - val_loss: 0.6595 - val_acc: 0.7556\n",
            "Epoch 14/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5689 - acc: 0.7864 - val_loss: 0.7366 - val_acc: 0.7391\n",
            "Epoch 15/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5626 - acc: 0.7892 - val_loss: 0.6619 - val_acc: 0.7409\n",
            "Epoch 16/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5523 - acc: 0.7920 - val_loss: 0.5881 - val_acc: 0.7744\n",
            "Epoch 17/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5410 - acc: 0.7959 - val_loss: 0.5646 - val_acc: 0.7837\n",
            "Epoch 18/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5362 - acc: 0.7973 - val_loss: 0.6000 - val_acc: 0.7747\n",
            "Epoch 19/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5291 - acc: 0.8000 - val_loss: 0.7616 - val_acc: 0.7013\n",
            "Epoch 20/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5392 - acc: 0.7970 - val_loss: 0.5564 - val_acc: 0.7839\n",
            "Epoch 21/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5145 - acc: 0.8046 - val_loss: 0.5900 - val_acc: 0.7758\n",
            "Epoch 22/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5108 - acc: 0.8057 - val_loss: 0.5501 - val_acc: 0.7863\n",
            "Epoch 23/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5060 - acc: 0.8072 - val_loss: 0.5383 - val_acc: 0.7901\n",
            "Epoch 24/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5005 - acc: 0.8090 - val_loss: 0.5695 - val_acc: 0.7766\n",
            "Epoch 25/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.5002 - acc: 0.8093 - val_loss: 0.7265 - val_acc: 0.7473\n",
            "Epoch 26/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4966 - acc: 0.8105 - val_loss: 0.5363 - val_acc: 0.7911\n",
            "Epoch 27/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4906 - acc: 0.8122 - val_loss: 0.5230 - val_acc: 0.7947\n",
            "Epoch 28/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4836 - acc: 0.8148 - val_loss: 0.5260 - val_acc: 0.7939\n",
            "Epoch 29/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4820 - acc: 0.8154 - val_loss: 0.5329 - val_acc: 0.7916\n",
            "Epoch 30/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4784 - acc: 0.8166 - val_loss: 0.5557 - val_acc: 0.7814\n",
            "Epoch 31/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4780 - acc: 0.8167 - val_loss: 0.5636 - val_acc: 0.7818\n",
            "Epoch 32/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4726 - acc: 0.8189 - val_loss: 0.5515 - val_acc: 0.7861\n",
            "Epoch 33/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4712 - acc: 0.8191 - val_loss: 0.5435 - val_acc: 0.7888\n",
            "Epoch 34/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4672 - acc: 0.8207 - val_loss: 0.5070 - val_acc: 0.8008\n",
            "Epoch 35/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4651 - acc: 0.8216 - val_loss: 0.5297 - val_acc: 0.7929\n",
            "Epoch 36/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4632 - acc: 0.8220 - val_loss: 0.5371 - val_acc: 0.7899\n",
            "Epoch 37/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4597 - acc: 0.8238 - val_loss: 0.5329 - val_acc: 0.7892\n",
            "Epoch 38/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4585 - acc: 0.8243 - val_loss: 0.5578 - val_acc: 0.7867\n",
            "Epoch 39/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4556 - acc: 0.8256 - val_loss: 0.5097 - val_acc: 0.7998\n",
            "Epoch 40/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4551 - acc: 0.8255 - val_loss: 0.4954 - val_acc: 0.8050\n",
            "Epoch 41/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4508 - acc: 0.8271 - val_loss: 0.5252 - val_acc: 0.7972\n",
            "Epoch 42/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4486 - acc: 0.8283 - val_loss: 0.4945 - val_acc: 0.8051\n",
            "Epoch 43/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4466 - acc: 0.8289 - val_loss: 0.5000 - val_acc: 0.8030\n",
            "Epoch 44/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4460 - acc: 0.8294 - val_loss: 0.5203 - val_acc: 0.7962\n",
            "Epoch 45/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4422 - acc: 0.8308 - val_loss: 0.5552 - val_acc: 0.7860\n",
            "Epoch 46/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4443 - acc: 0.8300 - val_loss: 0.4956 - val_acc: 0.8040\n",
            "Epoch 47/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4395 - acc: 0.8319 - val_loss: 0.5078 - val_acc: 0.8014\n",
            "Epoch 48/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4391 - acc: 0.8321 - val_loss: 0.4935 - val_acc: 0.8064\n",
            "Epoch 49/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4359 - acc: 0.8336 - val_loss: 0.4896 - val_acc: 0.8080\n",
            "Epoch 50/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4340 - acc: 0.8342 - val_loss: 0.4829 - val_acc: 0.8089\n",
            "Epoch 51/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4337 - acc: 0.8343 - val_loss: 0.4864 - val_acc: 0.8080\n",
            "Epoch 52/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4316 - acc: 0.8352 - val_loss: 0.5085 - val_acc: 0.8013\n",
            "Epoch 53/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4298 - acc: 0.8360 - val_loss: 0.4791 - val_acc: 0.8114\n",
            "Epoch 54/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4272 - acc: 0.8372 - val_loss: 0.5093 - val_acc: 0.8014\n",
            "Epoch 55/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4278 - acc: 0.8367 - val_loss: 0.4827 - val_acc: 0.8103\n",
            "Epoch 56/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4265 - acc: 0.8373 - val_loss: 0.4834 - val_acc: 0.8100\n",
            "Epoch 57/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4250 - acc: 0.8381 - val_loss: 0.5064 - val_acc: 0.8031\n",
            "Epoch 58/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4262 - acc: 0.8374 - val_loss: 0.4946 - val_acc: 0.8069\n",
            "Epoch 59/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4224 - acc: 0.8391 - val_loss: 0.5144 - val_acc: 0.8006\n",
            "Epoch 60/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4210 - acc: 0.8395 - val_loss: 0.5135 - val_acc: 0.8000\n",
            "Epoch 61/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4220 - acc: 0.8392 - val_loss: 0.4892 - val_acc: 0.8080\n",
            "Epoch 62/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4186 - acc: 0.8409 - val_loss: 0.4780 - val_acc: 0.8128\n",
            "Epoch 63/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4184 - acc: 0.8410 - val_loss: 0.4760 - val_acc: 0.8127\n",
            "Epoch 64/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4172 - acc: 0.8412 - val_loss: 0.4752 - val_acc: 0.8140\n",
            "Epoch 65/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4153 - acc: 0.8421 - val_loss: 0.4711 - val_acc: 0.8151\n",
            "Epoch 66/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4139 - acc: 0.8425 - val_loss: 0.5022 - val_acc: 0.8028\n",
            "Epoch 67/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4152 - acc: 0.8420 - val_loss: 0.4722 - val_acc: 0.8149\n",
            "Epoch 68/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4117 - acc: 0.8435 - val_loss: 0.4916 - val_acc: 0.8081\n",
            "Epoch 69/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4119 - acc: 0.8435 - val_loss: 0.4723 - val_acc: 0.8138\n",
            "Epoch 70/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4119 - acc: 0.8434 - val_loss: 0.5046 - val_acc: 0.8057\n",
            "Epoch 71/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4091 - acc: 0.8450 - val_loss: 0.4754 - val_acc: 0.8135\n",
            "Epoch 72/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4091 - acc: 0.8447 - val_loss: 0.4846 - val_acc: 0.8104\n",
            "Epoch 73/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4077 - acc: 0.8452 - val_loss: 0.4925 - val_acc: 0.8098\n",
            "Epoch 74/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4068 - acc: 0.8456 - val_loss: 0.4760 - val_acc: 0.8131\n",
            "Epoch 75/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4058 - acc: 0.8462 - val_loss: 0.4718 - val_acc: 0.8158\n",
            "Epoch 76/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4060 - acc: 0.8459 - val_loss: 0.4748 - val_acc: 0.8148\n",
            "Epoch 77/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4039 - acc: 0.8469 - val_loss: 0.4794 - val_acc: 0.8138\n",
            "Epoch 78/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4031 - acc: 0.8472 - val_loss: 0.4682 - val_acc: 0.8169\n",
            "Epoch 79/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4025 - acc: 0.8474 - val_loss: 0.4711 - val_acc: 0.8165\n",
            "Epoch 80/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4013 - acc: 0.8480 - val_loss: 0.5686 - val_acc: 0.7815\n",
            "Epoch 81/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4130 - acc: 0.8438 - val_loss: 0.4662 - val_acc: 0.8171\n",
            "Epoch 82/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.4004 - acc: 0.8483 - val_loss: 0.4654 - val_acc: 0.8172\n",
            "Epoch 83/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3986 - acc: 0.8491 - val_loss: 0.4822 - val_acc: 0.8137\n",
            "Epoch 84/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3991 - acc: 0.8488 - val_loss: 0.4635 - val_acc: 0.8188\n",
            "Epoch 85/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3974 - acc: 0.8498 - val_loss: 0.4633 - val_acc: 0.8185\n",
            "Epoch 86/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3965 - acc: 0.8500 - val_loss: 0.5050 - val_acc: 0.8069\n",
            "Epoch 87/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3971 - acc: 0.8498 - val_loss: 0.4723 - val_acc: 0.8171\n",
            "Epoch 88/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3948 - acc: 0.8509 - val_loss: 0.4640 - val_acc: 0.8193\n",
            "Epoch 89/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3944 - acc: 0.8511 - val_loss: 0.4655 - val_acc: 0.8190\n",
            "Epoch 90/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3936 - acc: 0.8514 - val_loss: 0.4668 - val_acc: 0.8183\n",
            "Epoch 91/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3930 - acc: 0.8514 - val_loss: 0.4725 - val_acc: 0.8166\n",
            "Epoch 92/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3929 - acc: 0.8516 - val_loss: 0.4711 - val_acc: 0.8171\n",
            "Epoch 93/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3920 - acc: 0.8518 - val_loss: 0.4719 - val_acc: 0.8170\n",
            "Epoch 94/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3921 - acc: 0.8518 - val_loss: 0.4622 - val_acc: 0.8197\n",
            "Epoch 95/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3906 - acc: 0.8525 - val_loss: 0.4643 - val_acc: 0.8201\n",
            "Epoch 96/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3893 - acc: 0.8531 - val_loss: 0.4661 - val_acc: 0.8196\n",
            "Epoch 97/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3895 - acc: 0.8530 - val_loss: 0.4875 - val_acc: 0.8098\n",
            "Epoch 98/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3895 - acc: 0.8530 - val_loss: 0.4697 - val_acc: 0.8163\n",
            "Epoch 99/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3880 - acc: 0.8538 - val_loss: 0.4643 - val_acc: 0.8202\n",
            "Epoch 100/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3877 - acc: 0.8538 - val_loss: 0.4656 - val_acc: 0.8192\n",
            "Epoch 101/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3868 - acc: 0.8543 - val_loss: 0.4671 - val_acc: 0.8176\n",
            "Epoch 102/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3862 - acc: 0.8545 - val_loss: 0.4740 - val_acc: 0.8160\n",
            "Epoch 103/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3864 - acc: 0.8546 - val_loss: 0.4726 - val_acc: 0.8179\n",
            "Epoch 104/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3854 - acc: 0.8546 - val_loss: 0.4595 - val_acc: 0.8209\n",
            "Epoch 105/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3842 - acc: 0.8553 - val_loss: 0.4617 - val_acc: 0.8207\n",
            "Epoch 106/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3836 - acc: 0.8556 - val_loss: 0.4888 - val_acc: 0.8132\n",
            "Epoch 107/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3844 - acc: 0.8550 - val_loss: 0.4630 - val_acc: 0.8202\n",
            "Epoch 108/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3829 - acc: 0.8557 - val_loss: 0.4566 - val_acc: 0.8225\n",
            "Epoch 109/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3820 - acc: 0.8561 - val_loss: 0.4584 - val_acc: 0.8213\n",
            "Epoch 110/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3824 - acc: 0.8561 - val_loss: 0.4583 - val_acc: 0.8227\n",
            "Epoch 111/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3812 - acc: 0.8565 - val_loss: 0.4613 - val_acc: 0.8216\n",
            "Epoch 112/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3815 - acc: 0.8565 - val_loss: 0.4611 - val_acc: 0.8216\n",
            "Epoch 113/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3799 - acc: 0.8571 - val_loss: 0.4615 - val_acc: 0.8215\n",
            "Epoch 114/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3791 - acc: 0.8575 - val_loss: 0.4585 - val_acc: 0.8227\n",
            "Epoch 115/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3793 - acc: 0.8575 - val_loss: 0.4579 - val_acc: 0.8212\n",
            "Epoch 116/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3787 - acc: 0.8578 - val_loss: 0.4615 - val_acc: 0.8199\n",
            "Epoch 117/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3781 - acc: 0.8579 - val_loss: 0.4604 - val_acc: 0.8220\n",
            "Epoch 118/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3775 - acc: 0.8582 - val_loss: 0.4641 - val_acc: 0.8218\n",
            "Epoch 119/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3771 - acc: 0.8582 - val_loss: 0.4637 - val_acc: 0.8217\n",
            "Epoch 120/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3765 - acc: 0.8586 - val_loss: 0.4801 - val_acc: 0.8124\n",
            "Epoch 121/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3770 - acc: 0.8583 - val_loss: 0.4901 - val_acc: 0.8145\n",
            "Epoch 122/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3762 - acc: 0.8587 - val_loss: 0.4580 - val_acc: 0.8229\n",
            "Epoch 123/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3755 - acc: 0.8590 - val_loss: 0.4649 - val_acc: 0.8218\n",
            "Epoch 124/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3745 - acc: 0.8595 - val_loss: 0.4567 - val_acc: 0.8223\n",
            "Epoch 125/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3740 - acc: 0.8598 - val_loss: 0.4620 - val_acc: 0.8217\n",
            "Epoch 126/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3740 - acc: 0.8596 - val_loss: 0.4569 - val_acc: 0.8238\n",
            "Epoch 127/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3732 - acc: 0.8601 - val_loss: 0.4658 - val_acc: 0.8213\n",
            "Epoch 128/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3729 - acc: 0.8602 - val_loss: 0.4545 - val_acc: 0.8244\n",
            "Epoch 129/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3721 - acc: 0.8604 - val_loss: 0.4570 - val_acc: 0.8240\n",
            "Epoch 130/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3720 - acc: 0.8605 - val_loss: 0.4894 - val_acc: 0.8132\n",
            "Epoch 131/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3729 - acc: 0.8602 - val_loss: 0.4547 - val_acc: 0.8240\n",
            "Epoch 132/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3710 - acc: 0.8609 - val_loss: 0.4562 - val_acc: 0.8235\n",
            "Epoch 133/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3705 - acc: 0.8612 - val_loss: 0.4614 - val_acc: 0.8218\n",
            "Epoch 134/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3713 - acc: 0.8608 - val_loss: 0.4660 - val_acc: 0.8206\n",
            "Epoch 135/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3704 - acc: 0.8611 - val_loss: 0.4531 - val_acc: 0.8252\n",
            "Epoch 136/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3690 - acc: 0.8619 - val_loss: 0.4564 - val_acc: 0.8235\n",
            "Epoch 137/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3691 - acc: 0.8618 - val_loss: 0.4579 - val_acc: 0.8239\n",
            "Epoch 138/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3685 - acc: 0.8620 - val_loss: 0.4583 - val_acc: 0.8220\n",
            "Epoch 139/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3684 - acc: 0.8620 - val_loss: 0.4532 - val_acc: 0.8251\n",
            "Epoch 140/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3677 - acc: 0.8624 - val_loss: 0.4669 - val_acc: 0.8226\n",
            "Epoch 141/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3671 - acc: 0.8626 - val_loss: 0.4588 - val_acc: 0.8239\n",
            "Epoch 142/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3669 - acc: 0.8628 - val_loss: 0.4584 - val_acc: 0.8218\n",
            "Epoch 143/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3662 - acc: 0.8631 - val_loss: 0.4555 - val_acc: 0.8252\n",
            "Epoch 144/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3663 - acc: 0.8630 - val_loss: 0.4617 - val_acc: 0.8242\n",
            "Epoch 145/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3655 - acc: 0.8632 - val_loss: 0.4571 - val_acc: 0.8249\n",
            "Epoch 146/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3652 - acc: 0.8634 - val_loss: 0.4627 - val_acc: 0.8234\n",
            "Epoch 147/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3653 - acc: 0.8634 - val_loss: 0.4537 - val_acc: 0.8249\n",
            "Epoch 148/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3644 - acc: 0.8637 - val_loss: 0.4759 - val_acc: 0.8144\n",
            "Epoch 149/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3652 - acc: 0.8634 - val_loss: 0.4548 - val_acc: 0.8254\n",
            "Epoch 150/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3637 - acc: 0.8641 - val_loss: 0.4554 - val_acc: 0.8241\n",
            "Epoch 151/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3633 - acc: 0.8643 - val_loss: 0.4590 - val_acc: 0.8245\n",
            "Epoch 152/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3631 - acc: 0.8642 - val_loss: 0.4730 - val_acc: 0.8209\n",
            "Epoch 153/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3637 - acc: 0.8639 - val_loss: 0.4836 - val_acc: 0.8183\n",
            "Epoch 154/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3631 - acc: 0.8643 - val_loss: 0.4533 - val_acc: 0.8258\n",
            "Epoch 155/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3620 - acc: 0.8647 - val_loss: 0.4616 - val_acc: 0.8235\n",
            "Epoch 156/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3614 - acc: 0.8650 - val_loss: 0.4614 - val_acc: 0.8245\n",
            "Epoch 157/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3618 - acc: 0.8648 - val_loss: 0.4532 - val_acc: 0.8262\n",
            "Epoch 158/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3613 - acc: 0.8650 - val_loss: 0.4537 - val_acc: 0.8262\n",
            "Epoch 159/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3602 - acc: 0.8655 - val_loss: 0.4539 - val_acc: 0.8243\n",
            "Epoch 160/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3606 - acc: 0.8653 - val_loss: 0.4705 - val_acc: 0.8214\n",
            "Epoch 161/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3602 - acc: 0.8654 - val_loss: 0.4521 - val_acc: 0.8267\n",
            "Epoch 162/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3598 - acc: 0.8657 - val_loss: 0.4615 - val_acc: 0.8239\n",
            "Epoch 163/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3594 - acc: 0.8659 - val_loss: 0.4604 - val_acc: 0.8213\n",
            "Epoch 164/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3595 - acc: 0.8658 - val_loss: 0.4549 - val_acc: 0.8240\n",
            "Epoch 165/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3587 - acc: 0.8661 - val_loss: 0.4811 - val_acc: 0.8202\n",
            "Epoch 166/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3587 - acc: 0.8662 - val_loss: 0.4555 - val_acc: 0.8262\n",
            "Epoch 167/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3580 - acc: 0.8665 - val_loss: 0.4522 - val_acc: 0.8270\n",
            "Epoch 168/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3574 - acc: 0.8668 - val_loss: 0.4553 - val_acc: 0.8252\n",
            "Epoch 169/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3573 - acc: 0.8667 - val_loss: 0.4554 - val_acc: 0.8255\n",
            "Epoch 170/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3568 - acc: 0.8671 - val_loss: 0.4545 - val_acc: 0.8262\n",
            "Epoch 171/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3568 - acc: 0.8671 - val_loss: 0.4533 - val_acc: 0.8254\n",
            "Epoch 172/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3563 - acc: 0.8673 - val_loss: 0.4527 - val_acc: 0.8255\n",
            "Epoch 173/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3558 - acc: 0.8675 - val_loss: 0.4548 - val_acc: 0.8253\n",
            "Epoch 174/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3563 - acc: 0.8671 - val_loss: 0.4530 - val_acc: 0.8255\n",
            "Epoch 175/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3558 - acc: 0.8674 - val_loss: 0.4508 - val_acc: 0.8271\n",
            "Epoch 176/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3546 - acc: 0.8679 - val_loss: 0.4589 - val_acc: 0.8257\n",
            "Epoch 177/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3545 - acc: 0.8680 - val_loss: 0.4547 - val_acc: 0.8269\n",
            "Epoch 178/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3547 - acc: 0.8679 - val_loss: 0.4553 - val_acc: 0.8263\n",
            "Epoch 179/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3541 - acc: 0.8681 - val_loss: 0.4554 - val_acc: 0.8267\n",
            "Epoch 180/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3540 - acc: 0.8681 - val_loss: 0.4541 - val_acc: 0.8266\n",
            "Epoch 181/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3536 - acc: 0.8683 - val_loss: 0.4541 - val_acc: 0.8252\n",
            "Epoch 182/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3531 - acc: 0.8686 - val_loss: 0.4534 - val_acc: 0.8272\n",
            "Epoch 183/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3529 - acc: 0.8686 - val_loss: 0.4575 - val_acc: 0.8264\n",
            "Epoch 184/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3528 - acc: 0.8687 - val_loss: 0.4518 - val_acc: 0.8271\n",
            "Epoch 185/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3520 - acc: 0.8690 - val_loss: 0.4516 - val_acc: 0.8263\n",
            "Epoch 186/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3529 - acc: 0.8686 - val_loss: 0.4551 - val_acc: 0.8266\n",
            "Epoch 187/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3517 - acc: 0.8692 - val_loss: 0.4564 - val_acc: 0.8271\n",
            "Epoch 188/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3516 - acc: 0.8692 - val_loss: 0.4549 - val_acc: 0.8268\n",
            "Epoch 189/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3512 - acc: 0.8693 - val_loss: 0.4547 - val_acc: 0.8257\n",
            "Epoch 190/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3511 - acc: 0.8695 - val_loss: 0.4515 - val_acc: 0.8274\n",
            "Epoch 191/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3509 - acc: 0.8696 - val_loss: 0.4517 - val_acc: 0.8272\n",
            "Epoch 192/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3503 - acc: 0.8697 - val_loss: 0.4595 - val_acc: 0.8235\n",
            "Epoch 193/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3510 - acc: 0.8694 - val_loss: 0.4531 - val_acc: 0.8270\n",
            "Epoch 194/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3501 - acc: 0.8697 - val_loss: 0.4497 - val_acc: 0.8273\n",
            "Epoch 195/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3497 - acc: 0.8701 - val_loss: 0.4513 - val_acc: 0.8276\n",
            "Epoch 196/200\n",
            "1380/1380 [==============================] - 26s 18ms/step - loss: 0.3492 - acc: 0.8703 - val_loss: 0.4544 - val_acc: 0.8272\n",
            "Epoch 197/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3489 - acc: 0.8704 - val_loss: 0.4590 - val_acc: 0.8230\n",
            "Epoch 198/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3493 - acc: 0.8701 - val_loss: 0.4564 - val_acc: 0.8251\n",
            "Epoch 199/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3487 - acc: 0.8705 - val_loss: 0.4507 - val_acc: 0.8272\n",
            "Epoch 200/200\n",
            "1380/1380 [==============================] - 26s 19ms/step - loss: 0.3481 - acc: 0.8706 - val_loss: 0.4498 - val_acc: 0.8276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64UrjuyKDUj-",
        "colab_type": "code",
        "outputId": "1843a825-9d11-4752-94d9-a1df5f078f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "for key in ['loss', 'val_loss']:\n",
        "    plt.plot(hist1.history[key], label = key)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfnILtPUDW75",
        "colab_type": "code",
        "outputId": "a36f8679-ca0f-417e-bb4f-4ca1b90409c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred = model.predict(X_val)\n",
        "y_predi = np.argmax(y_pred, axis=3)\n",
        "y_testi = np.argmax(y_val, axis=3)\n",
        "print(y_testi.shape,y_predi.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(204, 224, 224) (204, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "956SUw-eDYyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def IoU(yi, y_predi):\n",
        "  ## mean Intersection over Union\n",
        "  ## Mean IoU = TP/(FN + TP + FP)\n",
        "  IoUs = []\n",
        "  Nclass = 7\n",
        "  for c in range(Nclass):\n",
        "    TP = np.sum((yi == c) & (y_predi == c))\n",
        "    FP = np.sum((yi != c) & (y_predi == c))\n",
        "    FN = np.sum((yi == c) & (y_predi != c)) \n",
        "    IoU = TP / float(TP + FP + FN)\n",
        "    print('class {:02.0f}: #TP={:6.0f}, #FP={:6.0f}, #FN={:5.0f}, IoU={:4.3f}'.format(c, TP, FP, FN, IoU))\n",
        "    IoUs.append(IoU)\n",
        "  mIoU = np.mean(IoUs)\n",
        "  print('Mean IoU: {:4.3f}'.format(mIoU))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEqenuDvDa7C",
        "colab_type": "code",
        "outputId": "6ae89bdb-cd15-4fb3-f92e-c0050eeaab98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "IoU(y_testi, y_predi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class 00: #TP=3098921, #FP=237970, #FN=125801, IoU=0.895\n",
            "class 01: #TP= 72785, #FP= 76966, #FN=195306, IoU=0.211\n",
            "class 02: #TP= 39790, #FP= 37094, #FN=153988, IoU=0.172\n",
            "class 03: #TP=670818, #FP=241909, #FN=206790, IoU=0.599\n",
            "class 04: #TP=657393, #FP=550637, #FN=569436, IoU=0.370\n",
            "class 05: #TP=2001031, #FP=502052, #FN=415233, IoU=0.686\n",
            "class 06: #TP=1930465, #FP=118073, #FN=98147, IoU=0.899\n",
            "Mean IoU: 0.547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fakv6HOTRifW",
        "colab_type": "code",
        "outputId": "d25e9164-dcc0-4ecc-9290-880f7992fc81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSu7PFRwRqVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/gdrive/My Drive/IDD Challenge/fcn.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}